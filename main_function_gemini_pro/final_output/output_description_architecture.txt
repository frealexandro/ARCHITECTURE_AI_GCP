## Creando un flujo de datos con GCP desde la terminal: De Excel a BigQuery 

Este tutorial te guiará paso a paso para construir un flujo de datos en GCP que extrae datos de un archivo Excel, los valida y los carga en BigQuery, utilizando únicamente la línea de comandos de GCP.

**Prerrequisitos:**

1. **Cuenta de Google Cloud Platform:** Con una cuenta activa y un proyecto creado.
2. **Google Cloud SDK:** Instalada y configurada en tu máquina local. [https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)
3. **Servicio de cuentas:** Con roles que permitan interactuar con Cloud Storage, Cloud Functions y BigQuery.  Te recomiendo "Storage Admin", "Cloud Functions Developer" y "BigQuery Admin" para este tutorial.
4. **Archivo Excel:** Con los datos que deseas cargar a BigQuery.

**Recomendaciones:**

* **Organización de archivos:** Es recomendable crear carpetas dentro del bucket para organizar los archivos de entrada, salida y código.
* **Manejo de errores:** Implementa un control de errores en tu código Python para registrar errores en Cloud Logging.
* **Pruebas:** Realiza pruebas unitarias y de integración para validar el correcto funcionamiento del flujo de datos.

**Posibles errores:**

* **Permisos insuficientes:** Asegúrate de que la cuenta de servicio tenga los permisos necesarios para cada servicio.
* **Dependencias faltantes:** Verifica que las bibliotecas Python necesarias estén incluidas en el archivo `requirements.txt`.
* **Errores en el código:** Depura el código Python para identificar y corregir errores en la lógica o en la sintaxis.
* **Límites de cuota:** Ten en cuenta los límites de cuota de Cloud Functions y BigQuery.

## **Pasos Detallados:**

**1. Configuración Inicial:**

1.1. **Autenticación:** Inicia sesión en tu cuenta de GCP desde la terminal.

```bash
gcloud auth login
```

1.2. **Selecciona el proyecto:**  Selecciona el proyecto en el que deseas trabajar.

```bash
gcloud config set project <TU_ID_DE_PROYECTO> 
```

**2. Cloud Storage:**

2.1 **Crea un bucket:**  Crea un bucket para almacenar el archivo Excel y el código de la función. Reemplaza `<NOMBRE_DEL_BUCKET>` con un nombre único.

```bash
gsutil mb -l us-central1 gs://<NOMBRE_DEL_BUCKET>
```

**3. Cloud Functions:**

3.1. **Crea un directorio para la función:**

```bash
mkdir <NOMBRE_DE_LA_FUNCION>
cd <NOMBRE_DE_LA_FUNCION>
```

3.2. **Crea un archivo `main.py`:** Reemplaza `<NOMBRE_DEL_BUCKET>`, `<NOMBRE_DEL_DATASET>` y `<NOMBRE_DE_LA_TABLA>` con los nombres que elegiste.

```python
from google.cloud import bigquery
from google.cloud import storage
import pandas as pd
import os

PROJECT_ID = os.environ['GCP_PROJECT'] 
BUCKET_NAME = '<NOMBRE_DEL_BUCKET>' 
DATASET_ID = '<NOMBRE_DEL_DATASET>'
TABLE_ID = '<NOMBRE_DE_LA_TABLA>'

def process_excel(event, context):
    """Cloud Function triggered by Cloud Storage events.

    Args:
        event (dict):  The dictionary with data specific to this type of event.
                      The `data` field contains a description of the event in
                      the Cloud Storage bucket.
        context (google.cloud.functions.Context): Metadata of triggering event.

    """
    file = event
    file_name = file['name']

    # Crea un cliente de Cloud Storage
    storage_client = storage.Client()

    # Obtén el bucket y el blob (archivo)
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(file_name)

    # Descarga el contenido del blob a un archivo temporal
    temp_file_path = f"/tmp/{file_name}"
    blob.download_to_filename(temp_file_path)

    # Lee el archivo Excel con pandas
    df = pd.read_excel(temp_file_path)

    # Realiza la validación de las columnas (ejemplo: verifica si hay 82 columnas)
    if len(df.columns) != 82:
        raise ValueError(f"Número de columnas incorrecto: {len(df.columns)}")

    # Configura el cliente de BigQuery
    bq_client = bigquery.Client(project=PROJECT_ID)

    # Define la referencia a la tabla de destino
    table_ref = bq_client.dataset(DATASET_ID).table(TABLE_ID)

    # Configura la carga de datos
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.EXCEL,
        autodetect=True,  # Detecta automáticamente el esquema
    )

    # Carga los datos en BigQuery
    with open(temp_file_path, 'rb') as source_file:
        job = bq_client.load_table_from_file(
            source_file,
            table_ref,
            job_config=job_config,
        )
    job.result()  # Espera a que la carga se complete

    print(f"Archivo {file_name} procesado y cargado en BigQuery.")
```

3.3. **Crea un archivo `requirements.txt`:**

```txt
google-cloud-bigquery
google-cloud-storage
pandas
openpyxl
```

3.4. **Despliega la función:** Reemplaza `<NOMBRE_DE_LA_FUNCION>` con el nombre que desees para tu función.

```bash
gcloud functions deploy <NOMBRE_DE_LA_FUNCION> \
  --runtime python39 \
  --trigger-resource gs://<NOMBRE_DEL_BUCKET> \
  --trigger-event google.storage.object.finalize \
  --source .
```


**4. BigQuery:**

4.1 **Crea un dataset:** 

```bash
bq --location=us-central1 mk <NOMBRE_DEL_DATASET>
```

4.2 **Crea una tabla (opcional):** Puedes crear una tabla vacía con el esquema definido previamente si lo prefieres. De lo contrario, BigQuery infiere el esquema del archivo Excel.


**5. Prueba del Flujo:**

Sube un archivo Excel llamado "archivo_1.xlsx" al bucket:

```bash
gsutil cp archivo_1.xlsx gs://<NOMBRE_DEL_BUCKET>/
```

**Testing:**

* **Cloud Logging:**
   - Ve a la consola de GCP -> Cloud Functions -> selecciona tu función -> Pestaña "Registros". 
   - Busca mensajes de error o éxito para verificar la ejecución de la función.

* **BigQuery:**
    - Ve a la consola de GCP -> BigQuery.
    - Busca tu dataset y tabla. 
    - Ejecuta una consulta simple para verificar que los datos se cargaron correctamente:
       ```sql
       SELECT * FROM `<NOMBRE_DEL_DATASET>.<NOMBRE_DE_LA_TABLA>` LIMIT 10;
       ```

**Recuerda:**

* Este código Python asume que el archivo Excel tiene un formato estándar. Ajusta el código si tu archivo tiene un formato diferente.
* Configura alertas y monitorea el rendimiento de tu función y recursos de GCP para asegurar un funcionamiento óptimo.
