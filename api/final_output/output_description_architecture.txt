## Building a GCP Data Flow from the Terminal: Excel to BigQuery

This guide outlines the steps to create a GCP workflow that processes Excel files and loads data into BigQuery using only the GCP command line, without enabling APIs.

**Assumptions:**

* You have a GCP project set up.
* The Google Cloud SDK is installed and configured ([https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)).
* Your service account has necessary permissions (Storage Admin, Cloud Functions Developer, BigQuery Admin recommended).
* Your "archivo_1.xlsx" file is ready.

**1. Prepare Your Environment:**

1.1. **Authenticate:**

   ```bash
   gcloud auth login
   ```

1.2. **Select Project:**

   ```bash
   gcloud config set project YOUR_PROJECT_ID
   ```

   Replace `YOUR_PROJECT_ID` with your actual project ID.

**2. Cloud Storage Setup:**

2.1. **Create Bucket:**

   ```bash
   gsutil mb -l us-central1 gs://bucket_experiment
   ```

**3. Cloud Function Development:**

3.1. **Create Function Directory:**

   ```bash
   mkdir bucket_experiment && cd bucket_experiment 
   ```

3.2. **Create `main.py`:**

   ```python
   from google.cloud import bigquery
   from google.cloud import storage
   import pandas as pd
   import os

   # Project ID - automatically retrieved from environment
   project_id = os.environ['GCP_PROJECT']
   dataset_id = 'experiment'
   table_id = 'tabla_experiment'

   def process_excel(event, context):
       """Cloud Function triggered by file upload to bucket.
       
       Args:
           event (dict): Event payload.
           context (google.cloud.functions.Context): Metadata for the event.
       """
       file = event
       bucket_name = file['bucket']
       file_name = file['name']
       
       # Construct the GCS URI
       gcs_uri = f'gs://{bucket_name}/{file_name}'
       
       # Download the file to a temporary location
       storage_client = storage.Client()
       bucket = storage_client.bucket(bucket_name)
       blob = bucket.blob(file_name)
       blob.download_to_filename('/tmp/temp.xlsx') 

       # Read the Excel file into a Pandas DataFrame
       df = pd.read_excel('/tmp/temp.xlsx') 
       
       if len(df.columns) != 82:
           raise ValueError(f"File {file_name} has incorrect number of columns: {len(df.columns)}")

       # Load data into BigQuery
       bq_client = bigquery.Client()
       table_ref = bq_client.dataset(dataset_id).table(table_id)
       job_config = bigquery.LoadJobConfig(
           source_format=bigquery.SourceFormat.EXCEL,
           autodetect=True, # Attempt to autodetect schema
           write_disposition='WRITE_TRUNCATE' # Replace existing data
       )
       job = bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config)
       job.result()  # Wait for job completion

       print(f"File {file_name} processed and loaded into {dataset_id}.{table_id}") 
   ```

3.3. **Create `requirements.txt`:**

   ```
   google-cloud-bigquery
   google-cloud-storage
   pandas
   openpyxl
   ```

3.4. **Deploy the Function:**

   ```bash
   gcloud functions deploy bucket_experiment \
       --runtime python39 \
       --trigger-resource gs://bucket_experiment \
       --trigger-event google.storage.object.finalize \
       --source . \
       --entry-point process_excel 
   ```

**4. BigQuery Configuration:**

4.1. **Create Dataset:**

   ```bash
   bq --location=us mk experiment
   ```

4.2. **Create Table (Initially Empty):** 

   We will let BigQuery autodetect the schema from the Excel file.  There's no need to create a table structure beforehand when using `autodetect=True` in the load job configuration. 

**5. Upload File and Test:**

5.1. **Upload Excel File:**

   ```bash
   gsutil cp archivo_1.xlsx gs://bucket_experiment/
   ```

5.2. **Monitor Execution:**

   * **Cloud Logging:** Check logs for the `bucket_experiment` function.
   * **BigQuery:** Verify data is in the `experiment.tabla_experiment` table.

**Important Considerations:**

* **Error Handling:** Implement robust error handling in `main.py`.
* **Schema Management:** For consistent schemas, define it explicitly in the load job configuration instead of using `autodetect`.
* **Permissions:** Verify the service account used by the Cloud Function has appropriate permissions for the bucket and BigQuery.
* **Security:** Follow the principle of least privilege when granting permissions. 
* **Monitoring and Alerting:** Set up monitoring and alerts to proactively identify and address issues with the data flow. 

This comprehensive guide provides a solid foundation for building a robust and automated data processing pipeline from Excel files to BigQuery using only the GCP command line. 
