['mkdir $FUNCTION_NAME\n     cd $FUNCTION_NAME', 'from google.cloud import storage\n      from google.cloud import bigquery\n      import pandas as pd\n\n      def process_excel(event, context):\n          """Función activada por Cloud Storage al subir un archivo."""\n          file = event\n          bucket_name = file[\'bucket\']\n          file_name = file[\'name\']\n\n          if not file_name.endswith(\'.xlsx\'):\n              print(f"Archivo no procesado: {file_name}")\n              return\n\n          # Descarga el archivo de Cloud Storage\n          storage_client = storage.Client()\n          bucket = storage_client.bucket(bucket_name)\n          blob = bucket.blob(file_name)\n          blob.download_to_filename(\'/tmp/data.xlsx\')\n\n          # Lee el archivo Excel con pandas\n          df = pd.read_excel(\'/tmp/data.xlsx\', engine=\'openpyxl\')\n\n          # Valida que el archivo tenga 30 columnas\n          if len(df.columns) != 30:\n              print(f"Archivo inválido: {file_name} no tiene 30 columnas")\n              return\n\n          # Carga los datos a BigQuery\n          client = bigquery.Client()\n          table_id = f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}"\n\n          job_config = bigquery.LoadJobConfig(\n              source_format=bigquery.SourceFormat.PARQUET,\n              autodetect=True,\n              write_disposition="WRITE_APPEND"\n          )\n          job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n          job.result() \n\n          print(f"Archivo {file_name} procesado y cargado a BigQuery")', 'gcloud functions deploy $FUNCTION_NAME \\\n          --runtime python38 \\\n          --trigger-resource gs://$BUCKET_NAME \\\n          --trigger-event google.storage.object.finalize \\\n          --location $LOCATION \\\n          --source .']