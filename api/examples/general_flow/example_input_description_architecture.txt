## Creating a data flow with GCP from the terminal: From Excel to BigQuery 

This tutorial will guide you step by step to build a data flow in GCP that extracts data from an Excel file, validates it, and loads it into BigQuery, using only the GCP command line.

**Prerequisites:**

1. **Google Cloud Platform Account:** With an active account and a created project.
2. **Google Cloud SDK:** Installed and configured on your local machine. [https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)
3. **Service accounts:** With roles that allow interaction with Cloud Storage, Cloud Functions, and BigQuery. I recommend "Storage Admin", "Cloud Functions Developer", and "BigQuery Admin" for this tutorial.
4. **Excel file:** With the data you want to load into BigQuery.

**Recommendations:**

* **File organization:** It's recommended to create folders within the bucket to organize input, output, and code files.
* **Error handling:** Implement error control in your Python code to log errors in Cloud Logging.
* **Testing:** Perform unit and integration tests to validate the correct functioning of the data flow.

**Possible errors:**

* **Insufficient permissions:** Ensure that the service account has the necessary permissions for each service.
* **Missing dependencies:** Verify that the required Python libraries are included in the `requirements.txt` file.
* **Code errors:** Debug the Python code to identify and correct errors in logic or syntax.
* **Quota limits:** Be aware of the quota limits for Cloud Functions and BigQuery.

## **Detailed Steps:**

**1. Initial Configuration:**

1.1. **Authentication:** Log in to your GCP account from the terminal.

```bash
gcloud auth login
```

1.2. **Select the project:**  Select the project you want to work on.

```bash
gcloud config set project datalake-analytics-339922
```

**2. Cloud Storage:**

2.1 **Create a bucket:**  Create a bucket to store the Excel file and the function code.

```bash
gsutil mb -l us-central1 gs://bucket_experiment
```

**3. Cloud Functions:**

3.1. **Create a directory for the function:**

```bash
mkdir bucket_experiment
```

```bash
cd bucket_experiment
```

3.2. **Create a `main.py` file:**

```python
from google.cloud import bigquery
from google.cloud import storage
import pandas as pd
import os

PROJECT_ID = os.environ['GCP_PROJECT'] # Automatically gets the project ID
DATASET_ID = 'experiment'
TABLE_ID = 'tabla_experiment'

def process_excel(event, context):
    # Get the information of the uploaded file
    file = event
    bucket_name = file['bucket']
    file_name = file['name']

    # Download the Excel file from Cloud Storage

    route = "gs://{0}/{1}".format(bucket_name, filename)

    # Read the Excel file with pandas
    df = pd.read_excel('route')

    # Perform column validation (example: verify if there are 82 columns)
    if len(df.columns) != 82:
        raise ValueError(f"Incorrect number of columns: {len(df.columns)}")

    # Set up the BigQuery client
    bq_client = bigquery.Client(project=PROJECT_ID)

    # Define the schema of the destination table (optional)
    # You can omit this part if the table already exists with the correct schema
    table_ref = bq_client.dataset(DATASET_ID).table(TABLE_ID)
    job_config = bigquery.LoadJobConfig()
    # Define the schema here (see BigQuery documentation)

    # Load the data into BigQuery
    job = bq_client.load_table_from_dataframe(
        df, table_ref, job_config=job_config
    )
    job.result()  # Wait for the load to complete

    print(f"File {file_name} processed and loaded into BigQuery.")
```

3.3. **Create a `requirements.txt` file:**

```txt
google-cloud-bigquery
google-cloud-storage
pandas
openpyxl
```

3.4. **Deploy the function:**

```bash
gcloud functions deploy bucket_experiment \
  --runtime python39 \
  --trigger-resource gs://bucket_experiment \
  --trigger-event google.storage.object.finalize \
  --source .
```

Replace `<FUNCTION_NAME>` with the name you desire. This command will create the Cloud Function and configure a trigger for it to run every time a file is uploaded to the bucket.

**4. BigQuery:**

4.1 **Create a dataset:** If you don't have one yet, create a dataset in BigQuery to store the table.

```bash
bq --location=us-central1 mk experiment
```

4.2 **Create a blank table**

4.2.1 **Create a blank file to define the blank schema**

```bash
touch schema.json
```

4.2 **Create a table with blank schema**

```bash
bq mk --table \
  experiment.table_experiment \
  schema.json
```

Replace `<DATASET_LOCATION>` with the location of the dataset (e.g., `US`) and `<DATASET_NAME>` with the name of the dataset.

**5. Test the Flow:**

Upload a new Excel file to the bucket at the path `gs://bucket_experiment/`. The Cloud Function will automatically trigger, process the file, and load the data into the BigQuery table.

```bash
gsutil cp file.xlsx gs://bucket_experiment/
```

**Testing:**

* **Verify in Cloud Logging:** Review the function logs to detect errors.
* **Query the table in BigQuery:** Run an SQL query in BigQuery to verify that the data has been loaded correctly.

**Remember:**

* **Security:**  Use the principle of least privilege when granting roles to the service account.
* **Monitoring:**  Set up alerts to monitor the functioning of the data flow.

This automated data flow will allow you to efficiently process and analyze data from Excel files in Google Cloud Platform.